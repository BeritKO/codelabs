{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BeritKO/codelabs/blob/main/Bootcamp_2026_Prac_2_PyTorch_and_MLP_training_(JK).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaRUS09GMlQ-"
      },
      "source": [
        "# **MUST 2026 Bootcamp: Introduction to PyTorch and network training**\n",
        "Presented by Johan Kruger (thanks to Ruan van der Spoel for the 2025 example)\n",
        "\n",
        "**What we'll cover**\n",
        "\n",
        "* A basic introduction to PyTorch\n",
        "* How to create and train a neural network\n",
        "* How to tweak some hyperparameters\n",
        "* Using MNIST dataset for classification\n",
        "* Some fun challenges"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Pytorch?**\n",
        "\n",
        "PyTorch is a deep learning framework that makes it easy to build, train, and experiment with neural networks. It’s flexible, fast, and works great with GPUs."
      ],
      "metadata": {
        "id": "W6WsssNjshuB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZA4gpzQNidQ"
      },
      "source": [
        "**Setting up the environment**\n",
        "* Go to your copy of the notebook.\n",
        "* Select Runtime->Change Runtime Type.\n",
        "* Select 'T4 GPU' as a hardware accelerator if it is available.\n",
        "* If a GPU option isn't available, it just means your training will be a little slower.\n",
        "* Now we are setup! Let's get into it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXIUQ3XqOWIs"
      },
      "source": [
        "**Imports**\n",
        "\n",
        "Now we need to import all the packages that we'll be using.\n",
        "\n",
        "We'll be importing the following:\n",
        "* \"torch\" the main PyTorch library.\n",
        "* \"torch.nn\" the preconfigured PyTorch neural network building blocks that we need.\n",
        "* \"torchvision\" which consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "* \"torchvision.transforms\" the image transforms referred to above.\n",
        "* \"matplotlib.pyplot\" a python package for doing MATLAB style plotting.\n",
        "* \"numpy\" which is a library that is used for fast numerical computations. Provides support for multi-dimensional arrays and mathematical functions.\n",
        "* \"copy\" a package to copy things\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDwBBu7KQQEU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN8f8swLQeQb"
      },
      "source": [
        "#Dataset\n",
        "\n",
        "We will be using the MNIST dataset for this practical. MNIST consists of 70,000 grayscale images of handwritten digits (0-9), each 28x28 pixels in size, along with their corresponding labels.\n",
        "\n",
        "Let's import the MNIST dataset from the torchvision library. We will then load a train, validation, and test set in Pytorch which is split into three steps:\n",
        "1. Grab the actual train and test dataset from the torchvision library. This will download the dataset from a specified URL, and also transform the dataset from Numpy arrays to Torch tensors.\n",
        "2. Split the train set into a train and validation set.\n",
        "3. Use a DataLoader to load these data sets for training/validation/testing.\n",
        "\n",
        "We split the training set into a train and validation set to evaluate the model's performance on unseen data during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlVGCzneXXXC",
        "outputId": "4b095a95-8dce-4498-999d-aec79c214d37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.1MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 495kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.58MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# The size of the mini-batches that the data will be split up into\n",
        "batch_size = 128\n",
        "# The size of the validation set, which comes from the training set samples\n",
        "validation_set_size = 10000\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root = '../data/', # Where to store the dataset locally, or get it if it's already beem downloaded\n",
        "                                          train = True, # This means we are grabbing the train set of MNIST\n",
        "                                          transform = transforms.ToTensor(), # Transform the numpy arrays to Torch tensors\n",
        "                                          download = True) # Data set will require downloading (we haven't downloaded it before)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root = '../data/',\n",
        "                                         train = False, # This means we are grabbing the test set of MNIST\n",
        "                                         transform = transforms.ToTensor())\n",
        "\n",
        "# Create a validation dataset from the training set\n",
        "validation_dataset = copy.deepcopy(train_dataset)\n",
        "\n",
        "# Split the training set\n",
        "validation_dataset.data = train_dataset.data[0:validation_set_size] # Split the data (the images)\n",
        "train_dataset.data = train_dataset.data[validation_set_size:]\n",
        "validation_dataset.targets = train_dataset.targets[0:validation_set_size] # Split the targets (the labels)\n",
        "train_dataset.targets = train_dataset.targets[validation_set_size:]\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, # Specify the data set for the data loader\n",
        "                                           batch_size = batch_size, # Set the batch size\n",
        "                                           shuffle = True) # Shuffle the training examples\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = validation_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = False) #No need to shuffle the validation set, we only use it for validation!\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size = batch_size,\n",
        "                                          shuffle = False) #No need to shuffle the test set, we only use it for evaluation!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJygNuJzchQS"
      },
      "source": [
        "**MNIST**\n",
        "\n",
        "MNIST has 70k samples in total. The train set has 60k, and the test set has 10k. We have split the train set into 50k samples for training, and 10k for validation.\n",
        "\n",
        "Let's check that our sizes are correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_A4GJyvc2QC"
      },
      "outputs": [],
      "source": [
        "print('Training:\\t', train_dataset.data.shape[0])\n",
        "print('Validation:\\t', validation_dataset.data.shape[0])\n",
        "print('Evaluation:\\t', test_dataset.data.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gnqUAtldiCc"
      },
      "source": [
        "**What does the data look like?**\n",
        "\n",
        "To get an idea of how the MNIST data we are working with looks like, we can plot a sample or two from the training set.\n",
        "\n",
        "For the title, we can use the target (label) associated with the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwcKhutodv2U"
      },
      "outputs": [],
      "source": [
        "n = 6\n",
        "for i in range(1, n + 1):\n",
        "    plt.subplot(2,3,i)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(train_dataset.data[i].reshape((28, 28)), cmap=\"gray\")\n",
        "    plt.title('Target: ' + str(int(train_dataset.targets[i])));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tspI9pWeDkM"
      },
      "source": [
        "#Setting up an architecture\n",
        "* With our datasets prepped and ready to go, we can now build an actual neural network\n",
        "* In PyTorch, it's best to build the network inside a python class, and then we can create an object of this class which serves as our model.\n",
        "* We'll setup a simple model which takes the 28x28 MNIST images as input, passes them to three hidden layers, and outputs a 10-class probability distribution as prediction\n",
        "* We'll use layers with a width of 200\n",
        "* As activation function, we'll use the Rectified Linear Unit (ReLU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZD-WU3jzpaI"
      },
      "outputs": [],
      "source": [
        "num_classes = 10 # The number of MNIST classes (the number of possible outputs)\n",
        "input_dim = 784 # 28x28 = 784 input features\n",
        "layer_width = 200 # The number of nodes/neurons per hidden layer\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    # The Linear layer, also called a fully connected or dense layer, perform the simple linear transformations y=xW+b\n",
        "    self.hidden_layer_1 = nn.Sequential(nn.Linear(input_dim, layer_width, bias=True), nn.ReLU()) # 784 in, 200 out, each input is then ReLU'd.\n",
        "    self.hidden_layer_2 = nn.Sequential(nn.Linear(layer_width, layer_width, bias=False), nn.ReLU()) # 200 in, 200 out, each output is then ReLU'd.\n",
        "    self.hidden_layer_3 = nn.Sequential(nn.Linear(layer_width, layer_width, bias=False), nn.ReLU()) # 200 in, 200 out, each output is then ReLU'd.\n",
        "    self.output_layer = nn.Linear(layer_width, num_classes, bias=False) # 200 in, 10 out, no activation function applied.\n",
        "\n",
        "  def forward(self, x, **kwargs):\n",
        "    x = x.reshape(x.size(0), -1) # Converts the 2D 28x28 tensor into a 784-length vector\n",
        "    out = self.hidden_layer_1(x) # Pass inputs to the first hidden layer\n",
        "    out = self.hidden_layer_2(out) # Pass to hidden layer 2\n",
        "    out = self.hidden_layer_3(out) # Pass to hidden layer 3\n",
        "    out = self.output_layer(out) # Pass to output layer\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-XN9PVp6GFm"
      },
      "source": [
        "**Create object of Model**\n",
        "\n",
        "Now we can create an object of the model, and also assign it to a specific device (CPU or GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2gZwo-86PaY"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # If the GPU is available, select it. Otherwise, select CPU\n",
        "print('You are using: ' + str(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBSxDf-i6iex"
      },
      "outputs": [],
      "source": [
        "neuralNet = Model().to(device) # Create an object of the model, and move it to the appropriate device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etfhAL1X6wsc"
      },
      "source": [
        "**Hyperparameters**\n",
        "\n",
        "We can now specify the hyperparameters our network will be using:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xW7uyGN64L6"
      },
      "outputs": [],
      "source": [
        "max_epochs = 10 # The maximum number of epochs to train the network for\n",
        "learning_rate = 0.01 # The step-size multiplier for each optimisation step\n",
        "model_path_torch = './model.pth' # Where to save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtRvmrq_8hSF"
      },
      "source": [
        "\n",
        "\n",
        "**Select optimiser and loss function**\n",
        "\n",
        "* With the model created, we can now select an optimiser and the loss function we wish to use.\n",
        "* We'll use Cross-Entropy loss and the ADAM optimiser to start with.\n",
        "* NOTE: We don't use softmax on the last layer of the model, because in PyTorch Cross-Entropy already implements a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uffuBuRd8_R7"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss() # Select cross-entropy loss as cost function\n",
        "optimiser = torch.optim.Adam(neuralNet.parameters(), lr=learning_rate) # Select ADAM as optimiser, and assign the learning rate and model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fU1N6Sc9Zkb"
      },
      "source": [
        "#Train and test the model\n",
        "* First we'll write a function to evaluate our model - meaning that we will find out what its accuracy and loss is for a specific dataset.\n",
        "* Then we'll write a training loop that trains the model for a set number of epochs. After each epoch, the model is evaluated on the train and validation set.\n",
        "* Finally, we'll test the model on the test set, once training has been completed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSSj_YIWANEu"
      },
      "source": [
        "**Evaluation function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-fSPd8r960X"
      },
      "outputs": [],
      "source": [
        "def evaluatePerformance(data_set_loader, model, loss_func):\n",
        "  per_batch_loss = [] # Empty list to store the loss of each batch\n",
        "  model.eval() # Put the model in evaluation mode. This disables things such as batchnorm and dropout layers during evaluation\n",
        "  with torch.no_grad(): # Doesn't store the gradients of activations passed through the model. This speeds up the computation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in data_set_loader: # Iterate through the data loader\n",
        "      # Move each image and target label to the appropriate device (if using GPU, this would move the batch of images to the VRAM)\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Pass the images through the model\n",
        "      outputs = model(images)\n",
        "      # Take the highest prediction from the model for each sample\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      # Keep track of the total number of samples processed. This is necessary for computing accuracy.\n",
        "      total += labels.size(0)\n",
        "      # See how many of the predictions match the actual label\n",
        "      correct += (predicted == labels).sum().item()\n",
        "      # Calculate the loss between the predictions and true values\n",
        "      loss = loss_func(outputs, labels)\n",
        "      # Add this batch's loss the the list\n",
        "      per_batch_loss.append(float(loss) * labels.size(0))\n",
        "      # Instead of storing just the loss per batch, we multiply the loss by the batch size.\n",
        "      # This ensures that when we compute the epoch loss, we properly weight each batch's contribution.\n",
        "      # Otherwise, batches of different sizes (which can happen if the last batch is smaller) would not be accounted correctly.\n",
        "\n",
        "  # Get the average loss for the epoch\n",
        "  epoch_loss = sum(per_batch_loss) / total\n",
        "\n",
        "  # Calculate the accuracy\n",
        "  accuracy = round(100 * (correct/total), 4)\n",
        "\n",
        "  # Return the results\n",
        "  return accuracy, epoch_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-SBkl_YAPlI"
      },
      "source": [
        "**Training loop**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlrYFi5GASqa"
      },
      "outputs": [],
      "source": [
        "def trainModel(model, num_epochs):\n",
        "  best_valid_accuracy = 0\n",
        "  best_epoch = 0\n",
        "  best_epoch_train_accuracy = 0\n",
        "  total_step = len(train_loader)\n",
        "\n",
        "  # Loop through all epochs.\n",
        "  for epoch in range(num_epochs):\n",
        "    # Put the model in training mode (enables things like batchnorm and dropout)\n",
        "    model.train()\n",
        "    print(f'Optimising epoch {epoch+1}...')\n",
        "    # Iterate over the batches\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "      # Move each image and target label to the appropriate device (if using GPU, this would move the batch of images to the VRAM)\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Reset the gradient of each parameter to zero\n",
        "      optimiser.zero_grad()\n",
        "\n",
        "      # Run the forward pass\n",
        "\n",
        "      # Pass the samples through the model\n",
        "      outputs = model(images)\n",
        "      # Calculate the loss\n",
        "      loss = criterion(outputs, labels)\n",
        "\n",
        "      # Backward pass and optimise\n",
        "\n",
        "      # Backpropagate, calculate the gradients\n",
        "      loss.backward()\n",
        "      # Update parameters using gradients with optimiser\n",
        "      optimiser.step()\n",
        "\n",
        "    # Test the model on the training set\n",
        "    train_accuracy, train_loss = evaluatePerformance(train_loader, model, criterion)\n",
        "    print(f'Train Accuracy: {train_accuracy}%')\n",
        "    print(f'Train Loss: {train_loss}')\n",
        "    # Test the model on the validation set\n",
        "    validation_accuracy, validation_loss = evaluatePerformance(val_loader, model, criterion)\n",
        "    print(f'Validation Accuracy: {validation_accuracy}%')\n",
        "    print(f'Validation Loss: {validation_loss}')\n",
        "    print('\\n')\n",
        "\n",
        "    # Save the model at this epoch if it has performed better\n",
        "    if (validation_accuracy>best_valid_accuracy):\n",
        "      best_valid_accuracy = validation_accuracy\n",
        "      best_epoch = epoch+1\n",
        "      best_epoch_train_accuracy = train_accuracy\n",
        "      # Save the model state to the specified path\n",
        "      torch.save(model.state_dict(), model_path_torch)\n",
        "  # After training, test the best model on the test set\n",
        "\n",
        "  # Load best model\n",
        "  model.load_state_dict(torch.load(model_path_torch, weights_only=False))\n",
        "  # Test the model on the test set\n",
        "  test_accuracy, test_loss = evaluatePerformance(test_loader, model, criterion)\n",
        "\n",
        "  print(f'Best model at epoch {best_epoch}')\n",
        "  print(f'Train Accuracy: {best_epoch_train_accuracy}%')\n",
        "  print(f'Validation Accuracy: {best_valid_accuracy}%')\n",
        "  print(f'Evaluation Accuracy: {test_accuracy}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aYMgufcGZeE"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFUjTu7GGdM4"
      },
      "outputs": [],
      "source": [
        "trainModel(neuralNet, max_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WIsgZm8JJ6I"
      },
      "source": [
        "#Challenges\n",
        "1. Replace the ReLU activation function in the hidden layers with a different activation function e.g., Sigmoid or Tanh.\n",
        "2. Add a batch norm layer to each hidden layer.\n",
        "3. Add weight decay to the model (this is done via the optimiser). Compare the following weight decay values: 0.01, 0.0001, 0.000001.\n",
        "4. Change the optimiser to SGD.\n",
        "5. Add momentum to SGD and compare it to Adam.\n",
        "6. Add an additional hidden layer. You can choose any size!\n",
        "7. How does the accuracy look if you train the model on only 100 training samples?\n",
        "\n",
        "BONUS: Play with all hyperparameters (model size, optimiser, learning rate, batch size, etc.) and see who can get the best evaluation accuracy within 20 epochs.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}